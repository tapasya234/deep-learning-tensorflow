{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16af2145",
   "metadata": {},
   "source": [
    "# Building Blocks of Neural Networks\n",
    "\n",
    "## Loss Functions for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a542fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868cbdc",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum^n_{i=1}{y_{i} - y'_{i}}^2\n",
    "$$\n",
    "\n",
    "where \n",
    "`n`: number of examples or samples.\n",
    "`y`: true values.\n",
    "`y'`: predicted values.\n",
    "\n",
    "A smaller value indicates that the ground truth and predicted values are closer to each other.\n",
    "\n",
    "However, there a few disadvantages to using MSE: It is senstive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fbfa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9f0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc MSE Loss:  tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "Keras MSE Loss:  tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "Keras MSE Loss:  tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.Variable([0.0, 1.0, 0.0, 0.0])\n",
    "y_pred = tf.Variable([1.0, 1.0, 1.0, 0.0])\n",
    "print(\"Calc MSE Loss: \", mse_loss(y_true, y_pred))\n",
    "\n",
    "tf_mse = keras.losses.MeanSquaredError()\n",
    "print(\"Keras MSE Loss: \", tf_mse(y_true, y_pred))\n",
    "print(\"Keras MSE Loss: \", keras.losses.mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d8d69",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum^n_{i=1}{|y_{i} - y'_{i}|}\n",
    "$$\n",
    "\n",
    "where \n",
    "`n`: number of examples or samples.\n",
    "`y`: true values.\n",
    "`y'`: predicted values.\n",
    "\n",
    "A smaller value indicates that the ground truth and predicted values are closer to each other.\n",
    "\n",
    "It overcomes a disadvantages of MSE: Less sensitive to outliers compared to MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1863bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebe388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc MAE Loss:  tf.Tensor(1.75, shape=(), dtype=float32)\n",
      "Keras MAE Loss:  tf.Tensor(1.75, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.Variable([0.0, 2.0, 0.0, 3.0])\n",
    "y_pred = tf.Variable([1.0, 1.0, 2.0, 0.0])\n",
    "print(\"Calc MAE Loss: \", mae_loss(y_true, y_pred))\n",
    "print(\"Keras MAE Loss: \", keras.losses.mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18354a8f",
   "metadata": {},
   "source": [
    "#### Outliers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ccdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras MSE Loss:  tf.Tensor(10.5, shape=(), dtype=float32)\n",
      "Keras MAE Loss:  tf.Tensor(2.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.Variable([0.0, 2.0, 0.0, 7.0])\n",
    "y_pred = tf.Variable([1.0, 1.0, 2.0, 1.0])\n",
    "\n",
    "print(\"Keras MSE Loss: \", keras.losses.mean_squared_error(y_true, y_pred))\n",
    "print(\"Keras MAE Loss: \", keras.losses.mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022615",
   "metadata": {},
   "source": [
    "### MSE or MAE? Which loss function should be used?\n",
    "\n",
    "As shown, MSE applies square on each error term, and because of which the error value will get amplified because of any outliers. This is usually a good property, as it helps the model achieve better performance on the overall dataset by reducing significant deviations. In case of MAE, it doesn't highlight the outliers in the error. If the model needs to consider outlier predictions, then MAE wonâ€™t be as effective, as large errors coming from the outliers end up being weighted the same as lower errors. This might result in the model being great most of the time, but making a few very poor predictions as well!\n",
    "\n",
    "When a model is trained, if the MAE and MSE both stop at 30.5, then it's better to use MSE, as MSE penalizes larger errors more heavily than MAE because it squares the errors. This makes MSE models more sensitive to outliers. If the MSE-trained model converges to the same 30.5 error level as the MAE, it suggests that it has managed to handle outliers effectively, balancing both small and large errors well.\n",
    "\n",
    "Most optimizers use differentiation to find the optimum value for parameters in the evaluation metric. While, MSE is differentiable, as it's a continious function (resembles a parabola), MAE is not differentialble as zero. Since Modulo function is discreet, which make optimization with gradient descent more challenging and less efficient.\n",
    "\n",
    "MSE also helps to converge on the optimization problem faster compared to MAE, due to the quadratic nature of the loss function.\n",
    "\n",
    "so, in most cases, MSE is preffered over MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7466b",
   "metadata": {},
   "source": [
    "### Efficiency of MSE over MAE\n",
    "\n",
    "MSE loss function is faster to compute than the MAE loss function because\n",
    "\n",
    "- **Squaring is computationally faster**: The reason for this is that the MSE involves squaring the differences between predicted and target values, whereas the MAE involves taking the absolute differences. Computing the square of a number is generally faster than computing its absolute value, especially on modern processors that have optimized hardware for multiplication. \n",
    "\n",
    "- **Gradient**: When calculating the gradient for backpropagation during training, the derivative of the squared term in MSE is simpler and can be computed more efficiently.\n",
    "\n",
    "When working with increasingly large batches of data during training, the computational advantage of MSE over MAE becomes more significant. This is because the computation time of MSE depends on the size of the batch, while the computation time of MAE remains relatively constant regardless of the batch size.\n",
    "\n",
    "For example, if you have a batch of size N, the MSE loss requires N multiplications (to compute the squared differences) and then a sum operation, which can be efficiently parallelized. On the other hand, the MAE loss requires N absolute value computations and a sum operation. As the batch size increases, the number of absolute value computations in MAE remains constant, while the number of squared differences computations in MSE increases linearly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tkVenv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
